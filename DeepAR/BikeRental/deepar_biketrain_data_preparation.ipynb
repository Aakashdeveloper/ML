{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DeepAR - Kaggle Bike Sharing Demand Dataset</h2>\n",
    "<h4>Prepare Bike Rental Data for DeepAR training</h4>  \n",
    "<quote>We will store the total rental count, registered rental count, and casual rental count data as a time series.</quote>\n",
    "<quote>For each of the targets <b>total, registered, casual</b>, following JSON is structured as: Start Time, Array of target values, optional dynamic features and categories</quote>\n",
    "<quote>Frequency of time series data (for example, hourly, daily, monthly and so forth) is specified using hyperparameter</quote>\n",
    "\n",
    "<h4>To download original dataset, sign-in and download from this link: https://www.kaggle.com/c/bike-sharing-demand/data</h4>\n",
    "<br>\n",
    "None of these features are used: ['season', 'holiday', 'workingday', 'weather', 'temp',\n",
    "       'atemp', 'humidity', 'windspeed']<br>\n",
    "       \n",
    "Start Time From: ['datetime'] <br>\n",
    "Target Feature: [<b>'count','registered','casual'</b>]<br>\n",
    "Frequency: 'Hourly' <br>\n",
    "\n",
    "Objective: <quote>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period (Ref: Kaggle.com)</quote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = ['count','registered','casual']\n",
    "\n",
    "# controls if categories (in this case we are using it to indicate one of the above rentals) needs to be\n",
    "# included in the training and test data\n",
    "with_categories = False\n",
    "\n",
    "# Set datetime column as index to work with data based on Date/Time\n",
    "df = pd.read_csv('train.csv', parse_dates=['datetime'],index_col=0)\n",
    "df_test = pd.read_csv('test.csv', parse_dates=['datetime'],index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum time and Maximum Time in Training CSV file\n",
    "df.index.min(),df.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum time and Maximum Time in test CSV file\n",
    "df_test.index.min(),df_test.index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01']['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to let DeepAR know how far in the future predictions can be made using prediction_length hyperparameter\n",
    "# Let's look at how many hours we need to predict in a month using test.csv data file\n",
    "hours_to_predict = []\n",
    "print ('Check maximum hours we need to predict')\n",
    "# Group by year,month\n",
    "predict_window = df_test.groupby([df_test.index.year,df_test.index.month])\n",
    "for i,x in predict_window:\n",
    "    delta = x.index.max() - x.index.min() \n",
    "    hours = np.ceil(delta.total_seconds()/3600)\n",
    "    hours_to_predict.append(hours)\n",
    "    print (\"{0}, Hours:{1}\".format(i, hours))\n",
    "\n",
    "print (\"Maximum Prediction Length in Hours: \", np.max(hours_to_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "    \n",
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predict_max = pd.Timestamp(\"2012-12-31 23:00:00\", freq=freq) # 2012-12-31 23:00 alt way..pd.datetime(2012,12,31,23,0,0)\n",
    "\n",
    "dt_dataset_start_time = pd.Timestamp(\"2011-01-01 00:00:00\", freq=freq)\n",
    "dt_dataset_end_time = pd.Timestamp(\"2012-12-19 23:00:00\", freq=freq)\n",
    "\n",
    "# use for model training\n",
    "# Start time is the first row provided by kaggle\n",
    "# Training TS end time ensures some data is withheld for model testing\n",
    "# 12 days worth of training data is withheld for testing\n",
    "dt_train_range = (dt_dataset_start_time,\n",
    "                  dt_dataset_end_time - datetime.timedelta(hours=12*24) )\n",
    "\n",
    "# Use entire data for testing\n",
    "# We can compare predicted values vs actual (i.e. last 12 days is withheld for testing and model hasn't seen that data)\n",
    "dt_test_range = (dt_dataset_start_time, \n",
    "                 dt_dataset_end_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predict_max,dt_predict_max+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there are gaps in timesteps\n",
    "def is_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return not dt_range.equals(df[start:end].index)\n",
    "\n",
    "def get_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return dt_range.difference(df[start:end].index)    \n",
    "\n",
    "# List timeseries with only NaNs\n",
    "# They can be removed\n",
    "def timeseries_with_only_nans(df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df[col].min()):\n",
    "            #print (col)\n",
    "            l.append(col)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_missing_steps(df, '2011-01-01 00:00:00', '2011-01-19 23:00:00','H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_missing_steps(df, '2011-01-01 00:00:00', '2011-01-19 23:00:00','H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-02 00:00:00':'2011-01-02 14:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-02 00:00:00':'2011-01-02 14:00:00']['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.resample('1h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-02 00:00:00':'2011-01-02 14:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-02 00:00:00':'2011-01-02 14:00:00']['count'].plot(title='Missing values in training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2012-01':'2012-02']['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[dt_test_range[0]:dt_test_range[1]]['count'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_test = []\n",
    "time_series_training = []\n",
    "\n",
    "for target in target_values:\n",
    "    time_series_test.append(df[dt_test_range[0]:dt_test_range[1]][target])\n",
    "    time_series_training.append(df[dt_train_range[0]:dt_train_range[1]][target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_test[0][:5],time_series_test[1][:5],time_series_test[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_training[0][:5],time_series_training[1][:5],time_series_training[2][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_test[0].plot(label='test')\n",
    "time_series_training[0].plot(label='train')#, ls=':')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]  \n",
    "\n",
    "def encode_dynamic_feat(dynamic_feat):  \n",
    "    l = []\n",
    "    for col in dynamic_feat:\n",
    "        assert (not dynamic_feat[col].isna().any()), col  + ' has NaN'             \n",
    "        l.append(dynamic_feat[col].tolist())\n",
    "    return l\n",
    "\n",
    "def series_to_obj(ts, cat=None, dynamic_feat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = encode_dynamic_feat(dynamic_feat)\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None, dynamic_feat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat, dynamic_feat))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(time_series_training[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_to_obj(time_series_training[0][:5],[0] if with_categories else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_to_jsonline(time_series_training[0][:5],[0] if with_categories else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "cat_idx = 0\n",
    "\n",
    "train_file_name = \"train.json\"\n",
    "test_file_name = \"test.json\"\n",
    "\n",
    "if with_categories:\n",
    "    train_file_name = \"train_with_categories.json\"\n",
    "    test_file_name = \"test_with_categories.json\"\n",
    "\n",
    "with open(train_file_name, 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts,[cat_idx] if with_categories else None).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "        cat_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_idx = 0\n",
    "with open(test_file_name, 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts,[cat_idx] if with_categories else None).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "        cat_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data.csv',index=True,index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in time_series_test:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in time_series_training:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
