{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DeepAR - Kaggle Bike Sharing Demand Dataset with Dynamic Features</h2>\n",
    "<h4>Prepare Bike Rental Data for DeepAR training</h4>  \n",
    "<quote>We will store the total rental count, registered rental count, and casual rental count data as a time series.</quote>\n",
    "<quote>For each of the targets <b>total, registered, casual</b>, following JSON is structured as: Start Time, Array of target values, optional dynamic features and categories</quote>\n",
    "<quote>Frequency of time series data (for example, hourly, daily, monthly and so forth) is specified using hyperparameter</quote>\n",
    "\n",
    "<h4>To download original dataset, sign-in and download from this link: https://www.kaggle.com/c/bike-sharing-demand/data</h4>\n",
    "<br>\n",
    "<b>Dynamic features used</b>: ['season', 'holiday', 'workingday', 'weather', 'temp',\n",
    "       'atemp', 'humidity', 'windspeed']<br>\n",
    "       \n",
    "Start Time From: ['datetime'] <br>\n",
    "Target Feature: [<b>'count','registered','casual'</b>]<br>\n",
    "Frequency: 'Hourly' <br>\n",
    "\n",
    "Objective: <quote>You are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period (Ref: Kaggle.com)</quote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_features = ['season', 'holiday', 'workingday', 'weather', 'temp', 'atemp', 'humidity', 'windspeed']\n",
    "target_values = ['count','registered','casual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy over from biketrain_data_preparation\n",
    "\n",
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predict_max = pd.Timestamp(\"2012-12-31 23:00:00\", freq=freq) # 2012-12-31 23:00 alt way..pd.datetime(2012,12,31,23,0,0)\n",
    "\n",
    "dt_dataset_start_time = pd.Timestamp(\"2011-01-01 00:00:00\", freq=freq)\n",
    "dt_dataset_end_time = pd.Timestamp(\"2012-12-19 23:00:00\", freq=freq)\n",
    "\n",
    "# use for model training\n",
    "# Start time is the first row provided by kaggle\n",
    "# Training TS end time ensures some data is withheld for model testing\n",
    "# 12 days worth of training data is withheld for testing\n",
    "dt_train_range = (dt_dataset_start_time,\n",
    "                  dt_dataset_end_time - datetime.timedelta(hours=12*24) )\n",
    "\n",
    "# Use entire data for testing\n",
    "# We can compare predicted values vs actual (i.e. last 12 days is withheld for testing and model hasn't seen that data)\n",
    "dt_test_range = (dt_dataset_start_time, \n",
    "                 dt_dataset_end_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if there are gaps in timesteps\n",
    "def is_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return not dt_range.equals(df[start:end].index)\n",
    "\n",
    "def get_missing_steps(df,start,end,freq='D'):\n",
    "    dt_range = pd.date_range(start=start,end=end,freq=freq)\n",
    "    return dt_range.difference(df[start:end].index)    \n",
    "\n",
    "# List timeseries with only NaNs\n",
    "# They can be removed\n",
    "def timeseries_with_only_nans(df):\n",
    "    l = []\n",
    "    for col in df.columns:\n",
    "        if pd.isna(df[col].min()):\n",
    "            #print (col)\n",
    "            l.append(col)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', parse_dates=['datetime'],index_col=0)\n",
    "df_test = pd.read_csv('test.csv', parse_dates=['datetime'],index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Features for future bike count that needs to be predicted is available in two separate files\n",
    "# train.csv = contains dynamic features for model training and evaluation (i.e. upto 19th of each month)\n",
    "# test.csv = contains dynamic features for predictions that we need to submit to kaggle. (i.e from 20th to end of each month)\n",
    "# For model training with dynamic features, we need to provide dynamic features for missing time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01']['temp'].plot(title='Temperature upto 19th day - train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['2011-01']['temp'].plot(title='Temperature from 20th day to end of month - test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge train.csv and test.csv files to a single dataframe\n",
    "# This will allow dynamic features for missing time steps to populated from test.csv file\n",
    "# NOTE: There are some missing time steps in test.csv as well...we need to handle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['2011-01'].index.min(),df_test['2011-01'].index.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing time steps in test.csv\n",
    "get_missing_steps(df_test,'2011-01-20','2011-01-31','H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's append a new row with max predicted date\n",
    "obj = {}\n",
    "for col in df.columns:\n",
    "    obj[col] = [np.nan]\n",
    "df = df.append(pd.DataFrame(obj,index=[dt_predict_max]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample entire data at 1 hour frequency\n",
    "df = df.resample('1h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset has these features set to NaN\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test file has features available for some of the time steps\n",
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's update data from test\n",
    "df.update(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have data for dynamic features\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every feature has NaN (missing values)\n",
    "# We need to fill in these missing features\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at some missing steps\n",
    "# Jan 2011\n",
    "get_missing_steps(df_test,'2011-01-20','2011-01-31','H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dec 2012\n",
    "get_missing_steps(df_test,'2012-12-20','2012-12-31','H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2012-12-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season']['2011-01'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season']['2012-12-17':'2012-12-25'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/19966018/pandas-filling-missing-values-by-mean-in-each-group\n",
    "# For these ['holiday','workingday','season'] columns, we simply need to look for another hour in the same day\n",
    "# for which value is available and use it to fillna\n",
    "# Holiday won't change hour to hour\n",
    "# Holiday,Workingday and Season are the same for all hours of the day\n",
    "group_ymd = df[['holiday','workingday','season']].groupby([df.index.year,df.index.month,df.index.day])\n",
    "\n",
    "for col in ['holiday','workingday','season']:\n",
    "    print(col)\n",
    "    df[col] = group_ymd[col].transform(lambda x: x.fillna(x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season']['2011-01'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season']['2012-12-17':'2012-12-25'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining NaN values in features, simply ffill\n",
    "for col in ['weather', 'temp', 'atemp', 'humidity', 'windspeed']:\n",
    "    df[col] = df[col].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2011-01-26']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['2012-12-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, only the rental count columns [count,registered,casual] should have NaN to indicate missing values\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dynamic_feat = df[dynamic_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dynamic_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dynamic_feat.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing target values that we need to predict and fill\n",
    "df['2012-01':'2012-02']['count'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_test = []\n",
    "time_series_training = []\n",
    "\n",
    "for t in target_values:\n",
    "    time_series_test.append(df[dt_test_range[0]:dt_test_range[1]][t])\n",
    "    time_series_training.append(df[dt_train_range[0]:dt_train_range[1]][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic features are the same for count, registered, casual\n",
    "dynamic_features_test = df_dynamic_feat [dt_test_range[0]:dt_test_range[1]]\n",
    "dynamic_features_training = df_dynamic_feat[dt_train_range[0]:dt_train_range[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_test[0].plot(label='test')\n",
    "time_series_training[0].plot(label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]  \n",
    "\n",
    "def encode_dynamic_feat(dynamic_feat):  \n",
    "    l = []\n",
    "    for col in dynamic_feat:\n",
    "        assert (not dynamic_feat[col].isna().any()), col  + ' has NaN'             \n",
    "        l.append(dynamic_feat[col].tolist())\n",
    "    return l\n",
    "\n",
    "def series_to_obj(ts, cat=None, dynamic_feat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = encode_dynamic_feat(dynamic_feat)\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None, dynamic_feat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat, dynamic_feat))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_to_jsonline(time_series_training[0][:5], dynamic_feat=dynamic_features_training[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "with open(\"train_dynamic_feat.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts,dynamic_feat=dynamic_features_training).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_dynamic_feat.json\", 'wb') as fp:\n",
    "    for ts in time_series_test:\n",
    "        fp.write(series_to_jsonline(ts,dynamic_feat=dynamic_features_test).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_data_dynamic_feat.csv',index=True,index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in time_series_test:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts in time_series_training:\n",
    "    print (len(ts),ts.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
