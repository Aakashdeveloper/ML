{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "\n",
    "# This code is derived from AWS SageMaker Samples:\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_electricity\n",
    "# https://github.com/awslabs/amazon-sagemaker-examples/tree/master/introduction_to_amazon_algorithms/deepar_synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide endpoint\n",
    "with_categories = True\n",
    "endpoint_name = 'deepar-biketrain-with-categories-2018-12-21-04-05-44-478'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq='H' # Timeseries consists Hourly Data and we need to predict hourly rental count\n",
    "\n",
    "# how far in the future predictions can be made\n",
    "# 12 days worth of hourly forecast \n",
    "prediction_length = 288 \n",
    "\n",
    "# aws recommends setting context same as prediction length as a starting point. \n",
    "# This controls how far in the past the network can see\n",
    "context_length = 288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_predict_max = pd.Timestamp(\"2012-12-31 23:00:00\", freq=freq) # 2012-12-31 23:00 alt way..pd.datetime(2012,12,31,23,0,0)\n",
    "\n",
    "dt_dataset_start_time = pd.Timestamp(\"2011-01-01 00:00:00\", freq=freq)\n",
    "dt_dataset_end_time = pd.Timestamp(\"2012-12-19 23:00:00\", freq=freq)\n",
    "\n",
    "# use for model training\n",
    "# Start time is the first row provided by kaggle\n",
    "# Training TS end time ensures some data is withheld for model testing\n",
    "# 12 days worth of training data is withheld for testing\n",
    "dt_train_range = (dt_dataset_start_time,\n",
    "                  dt_dataset_end_time - datetime.timedelta(hours=12*24) )\n",
    "\n",
    "# Use entire data for testing\n",
    "# We can compare predicted values vs actual (i.e. last 12 days is withheld for testing and model hasn't seen that data)\n",
    "dt_test_range = (dt_dataset_start_time, \n",
    "                 dt_dataset_end_time) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]  \n",
    "\n",
    "def encode_dynamic_feat(dynamic_feat):  \n",
    "    l = []\n",
    "    for col in dynamic_feat:\n",
    "        assert (not dynamic_feat[col].isna().any()), col  + ' has NaN'             \n",
    "        l.append(dynamic_feat[col].tolist())\n",
    "    return l\n",
    "\n",
    "def series_to_obj(ts, cat=None, dynamic_feat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = encode_dynamic_feat(dynamic_feat)\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None, dynamic_feat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat, dynamic_feat))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "    def set_prediction_parameters(self, freq, prediction_length):\n",
    "        \"\"\"Set the time frequency and prediction length parameters. This method **must** be called\n",
    "        before being able to use `predict`.\n",
    "        \n",
    "        Parameters:\n",
    "        freq -- string indicating the time frequency\n",
    "        prediction_length -- integer, number of predicted time points\n",
    "        \n",
    "        Return value: none.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.prediction_length = prediction_length\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                encoding=\"utf-8\", num_samples=100, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        Parameters:\n",
    "        ts -- list of `pandas.Series` objects, the time series to predict\n",
    "        cat -- list of integers (default: None)\n",
    "        encoding -- string, encoding to use for the request (default: \"utf-8\")\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_times = [x.index[-1]+1 for x in ts]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, encoding, num_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, prediction_times, encoding)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, encoding, num_samples, quantiles):\n",
    "        \n",
    "        instances = [series_to_obj(ts[k], \n",
    "                                   cat[k] if cat else None,\n",
    "                                   dynamic_feat if dynamic_feat else None) \n",
    "                     for k in range(len(ts))]\n",
    "        \n",
    "        configuration = {\"num_samples\": num_samples, \"output_types\": [\"quantiles\"], \"quantiles\": quantiles}\n",
    "        http_request_data = {\"instances\": instances, \"configuration\": configuration}\n",
    "        return json.dumps(http_request_data).encode(encoding)\n",
    "    \n",
    "    def __decode_response(self, response, prediction_times, encoding):\n",
    "        response_data = json.loads(response.decode(encoding))\n",
    "        list_of_df = []\n",
    "        for k in range(len(prediction_times)):\n",
    "            prediction_index = pd.DatetimeIndex(start=prediction_times[k], freq=self.freq, periods=self.prediction_length)\n",
    "            list_of_df.append(pd.DataFrame(data=response_data['predictions'][k]['quantiles'], index=prediction_index))\n",
    "        return list_of_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "predictor.set_prediction_parameters(freq, prediction_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all_data.csv', parse_dates=['datetime'],index_col=0)\n",
    "df_test = pd.read_csv('test.csv', parse_dates=['datetime'],index_col=0) # data points to be predicted for submission\n",
    "df = df.resample('1h').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = ['count','registered','casual']\n",
    "time_series_test = []\n",
    "time_series_training = []\n",
    "\n",
    "for t in target_values:\n",
    "    time_series_test.append(df[dt_test_range[0]:dt_test_range[1]][t])\n",
    "    time_series_training.append(df[dt_train_range[0]:dt_train_range[1]][t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide 0 based index for categories\n",
    "list_of_df = predictor.predict(time_series_training,\n",
    "                               cat=[[0],[1],[2]] if with_categories else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_of_df)):\n",
    "    print(len(list_of_df[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_df[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(list_of_df)):\n",
    "    # print (-prediction_length-context_length) #120 = 72+48\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    time_series_test[k][-prediction_length-context_length:].plot(label='target')\n",
    "    \n",
    "    p10 = list_of_df[k]['0.1']\n",
    "    p90 = list_of_df[k]['0.9']\n",
    "    plt.fill_between(p10.index, p10, p90, color='y', alpha=0.5, label='80% confidence interval')\n",
    "    list_of_df[k]['0.5'].plot(label='prediction median')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_window = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in df_test.groupby([df_test.index.year,df_test.index.month]):\n",
    "    predict_window.append(x.index.min()-datetime.timedelta(hours=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in target_values:\n",
    "    df_test[t] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for window in predict_window:\n",
    "    print(window)\n",
    "    # If trained with categories, we need to send corresponding category for each training set\n",
    "    # In this case\n",
    "    for i in range(len(target_values)):\n",
    "        list_of_df = predictor.predict([time_series_test[i][:window]],\n",
    "                                       cat=[i] if with_categories else None)\n",
    "        df_tmp = list_of_df[0]\n",
    "        df_tmp.index.name = 'datetime'\n",
    "        df_tmp.columns = ['0.1',target_values[i],'0.9']\n",
    "        df_test.update(df_tmp[target_values[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_count(x):\n",
    "    if x < 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['count'] = df_test['count'].map(adjust_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the results\n",
    "if with_categories:\n",
    "    df_test[['count']].to_csv('prediction-with-categories.csv',index=True,index_label='datetime')\n",
    "else:\n",
    "    df_test[['count']].to_csv('prediction.csv',index=True,index_label='datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint after completing the demo...otherwise, your account will accumulate hourly charges\n",
    "sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
